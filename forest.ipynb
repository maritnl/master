{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cb959-be36-4faf-b51e-0f7679d0f5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec692dd4-4678-4ea2-bd10-5c43fb95f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('leg234_data.csv')\n",
    "#Flip wrong datapoints\n",
    "df['LATITUDE'] = np.where((df['LATITUDE']>=10) & (df['date'] >= '2022-02-13'), -df['LATITUDE'], df['LATITUDE'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6c386-ae2e-41d5-9192-35ba9c0c7a3a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/impurity.png\" width=\"400\"/>\n",
    "<img src=\"images/permutation.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caed89bd-8147-4ed0-a409-6763c7369209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['NMEA.Wave_Height', 'PCO2.H2O_mmm', 'PCO2.atm_cond', 'NMEA.Wind_Speed', 'NMEA.Wind_Angle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71beb8-d431-4145-9b5b-f73baf87d1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sat = df[['LATITUDE', 'LONGITUD', 'PDMEAN', 'FerryBox.Optode_Saturation', 'time', 'NMEA.Wind_Angle', 'NMEA.Wind_Speed', 'TOTAL']]\n",
    "df_time = df[['LATITUDE', 'LONGITUD', 'PDMEAN', 'time', 'TOTAL']]\n",
    "df_wind = df[['LATITUDE', 'LONGITUD', 'PDMEAN', 'NMEA.Wind_Angle', 'TOTAL']]\n",
    "\n",
    "names = df_sat.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(df_sat)\n",
    "df_sat = pd.DataFrame(scaled_df, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32415941-1185-4260-a09b-bb95912f2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['LATITUDE', 'LONGITUD', 'PDMEAN', 'TOTAL']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff316457-a901-4e52-8949-c801504844f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:, df.columns == 'TOTAL']\n",
    "X = df.iloc[:, df.columns != 'TOTAL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "    test_size=0.25, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73475cc4-5e69-49d3-b591-189588b60856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(A, F):\n",
    "    return 1/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea235e-a0bc-4c29-bddb-ef91a36d7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(random_state=0)\n",
    "forest.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c22b6b-433c-414b-8489-e32c8571127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pred = forest.predict(X_val)\n",
    "print('MAE score: ', mean_absolute_error(y_val, forest_pred))\n",
    "print('R2 score: ', r2_score(y_val, forest_pred))\n",
    "print('Smape score: ', smape(y_val, forest_pred.reshape(41194,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4472ce9-80b5-48f4-ab87-b4574b24adc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameters currently in use:\\n')\n",
    "print(forest.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e18f1-1ca1-4016-81f6-76adaebe1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "{'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23335920-9c6d-4dc2-b410-7f0dd3a8e4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed33f81-551e-4d40-a678-dc98d7236fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0c385-f0af-4f16-84c5-f4ec0b1d8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad86b2-fcae-4cb9-8ae9-e9aff59284b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pred = best_random.predict(X_val)\n",
    "print('MAE score: ', mean_absolute_error(y_val, r_pred))\n",
    "print('R2 score: ', r2_score(y_val, r_pred))\n",
    "print('Smape score: ', smape(y_val, r_pred.reshape(41194,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4405ced-f7ae-4b76-8f79-839c54278f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor(n_estimators=50, random_state=0, criterion='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd2ffd-3f3d-430f-b5b1-f71dbcc37998",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77754df-0ba4-4661-a068-d01bb4c41f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pred = clf.predict(X_val)\n",
    "print('MAE score: ', mean_absolute_error(y_val, clf_pred))\n",
    "print('R2 score: ', r2_score(y_val, clf_pred))\n",
    "print('Smape score: ', smape(y_val, clf_pred.reshape(41194,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d29a7a-aae7-4a39-97f4-ea7da6636080",
   "metadata": {},
   "source": [
    "# Creating different datasets\n",
    "### North and South Atlantic\n",
    "### Max 600m depth\n",
    "### Keep rows with some NaNs (remove harbour dates and where all values are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff008d8-09d3-4e65-8d59-8d3b84e17a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter original dataframe. Return dataframe where lat / long is higher than given\n",
    "def df_by_position(df, min_lat = None, max_lat = None, min_long = None, max_long = None):\n",
    "    if min_lat:\n",
    "        df = df[df['LATITUDE'] >= min_lat] \n",
    "    if min_long: \n",
    "        df = df[df['LONGITUD'] >= min_long] \n",
    "    if max_lat:\n",
    "        df = df[df['LATITUDE'] <= max_lat] \n",
    "    if max_long:\n",
    "        df = df[df['LONGITUD'] <= max_long] \n",
    "    return df\n",
    "\n",
    "# Return df with max depth given\n",
    "def df_by_depth(df, max_depth = None):\n",
    "    if max_depth:\n",
    "        df = df[df['PDMEAN'] <= max_depth] \n",
    "    return df\n",
    "\n",
    "def df_specific_depth(df, depth):\n",
    "    df = df[df['PDMEAN'] == depth] \n",
    "    df = df[['LATITUDE','LONGITUD','PDMEAN', 'TOTAL', 'date']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77e186-ddf2-43b2-8a52-5aa5f528cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "north = df_by_position(df, min_lat=0)\n",
    "mid_atlantic = df_by_position(df, min_lat = -3, max_lat = 15, min_long = -44, max_long = -28)\n",
    "carib = df_by_position(df, min_lat = 15, max_lat = 25, min_long = -85, max_long = -70)\n",
    "shallow = df_by_depth(df, 600)\n",
    "depth = df_specific_depth(df, 545)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9776de-afda-46ca-9fbf-239294fc6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "south = df_by_position(df, max_lat=1)\n",
    "south"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd839614-d156-4181-96b5-9522857b2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = gpd.GeoDataFrame(\n",
    "    depth, geometry=gpd.points_from_xy(depth.LATITUDE, depth.LONGITUD))\n",
    "\n",
    "depth.reset_index(level=0, inplace=True)\n",
    "\n",
    "depth['date'] = depth['date'].astype(str)\n",
    "\n",
    "depth = depth.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61320f-134c-4022-95c5-f3a0e49618e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6020c-b326-413c-a7bb-b13a0f5ec4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig = px.scatter_geo(depth,lat='LATITUDE',lon='LONGITUD', hover_name=\"TOTAL\")\n",
    "fig = px.scatter_geo(depth,\n",
    "                    lat=depth.geometry.x,\n",
    "                    lon=depth.geometry.y,\n",
    "                    color=\"TOTAL\",\n",
    "                    size='TOTAL',\n",
    "                    hover_data=['TOTAL', 'date'])\n",
    "fig.update_geos(\n",
    "    center=dict(lon=-30, lat=10),\n",
    "    lataxis_range=[-30,30], lonaxis_range=[-100, 50]\n",
    ")\n",
    "fig.update_layout(title = 'Biomass at 545 meters depth', title_x=0.5, height=500,\n",
    "                 )\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892d4cd-a029-412c-a0a3-80f1a323f4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    y = df.iloc[:, df.columns == 'TOTAL']\n",
    "    X = df.iloc[:, df.columns != 'TOTAL']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n",
    "        test_size=0.25, random_state=123) \n",
    "    \n",
    "\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacb2ca-958d-4d73-b8aa-69efd8dcbf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_forest(X_train, y_train):\n",
    "    forest = RandomForestRegressor(random_state=0)\n",
    "    forest.fit(X_train, y_train.values.ravel())\n",
    "    return forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd44f6-b994-49ac-b915-6b8a3e668e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_score(X_val, y_val, forest):\n",
    "    r_pred = forest.predict(X_val)\n",
    "    print('MAE score: ', mean_absolute_error(y_val, r_pred))\n",
    "    print('R2 score: ', r2_score(y_val, r_pred))\n",
    "    print('Smape score: ', smape(y_val, r_pred.reshape(len(X_val),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1452b3-bcc6-4718-acbb-d728045c7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(shallow)\n",
    "forest = fit_forest(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbfa4ad-6243-41bc-a70b-a8886c8f7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_score(X_val, y_val, forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ecb36-5040-4709-b0c5-95ec60846ba3",
   "metadata": {},
   "source": [
    "# Removing dates when ship was in port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ef044-5674-4a1c-abdc-59d1209b22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Port':['La Coruna', 'Lisbon', 'Cadiz', 'Las Palmas', 'Willemstad',\n",
    "                'Port Royal', 'Havana', 'Nassau', 'Miami', 'New York',\n",
    "                'New Port', 'Horta', 'Rio'], 'Date from':['28.08.2021', '02.09.2021', '09.09.2021',\n",
    "                '30.09.2021', '04.11.2021', '13.11.2021', '24.11.2021', '02.12.2021', '07.12.2021',\n",
    "                '18.12.2021', '05.01.2022', '22.01.2022', '23.02.2022'],\n",
    "                'Date to':['29.08.2021', '05.09.2021', '16.09.2021', '04.10.2021', '08.11.2021','17.11.2021',\n",
    "                '28.11.2021', '05.12.2021', '10.12.2021', '04.01.2022', '08.01.2022', '24.01.2022', '26.02.2022']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f75dd-d206-4d6f-a6f3-41dfee1d5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ports = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b3a0d-3ba6-45b5-b218-b37bbd181f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ports['Date from'] = pd.to_datetime(df_ports['Date from'], dayfirst=True, format='%d.%m.%Y')\n",
    "df_ports['Date to'] = pd.to_datetime(df_ports['Date to'], dayfirst=True, format='%d.%m.%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c8f85-53cc-4535-b699-828e38dc116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ports_df(ports, df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    for idx, row in ports.iterrows():\n",
    "        from_date = row['Date from']\n",
    "        to_date = row['Date to']\n",
    "        #df = df.drop([(df['date'] <= from_date) & (df['date'] >= to_date)])\n",
    "        df = df[(df['date'] < from_date) | (df['date'] > to_date)]\n",
    "        \n",
    "    #Fill median, drop rows with missing EK data\n",
    "    df = df.dropna(subset=['TOTAL'])\n",
    "    df = df.drop(columns=['date'])\n",
    "    df = df.fillna(df.median())\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd8a02-9bc8-44b7-a9ba-e7bb3df62eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "south = remove_ports_df(df_ports, south)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc1101-996c-44f5-a017-856ec59b180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "carib = remove_ports_df(df_ports, carib)\n",
    "mid_atlantic = remove_ports_df(df_ports, mid_atlantic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301f471-17ed-48da-b569-a05f3c2c5839",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = mid_atlantic.columns\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_df = scaler.fit_transform(mid_atlantic)\n",
    "mid_atlantic = pd.DataFrame(scaled_df, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047a66e-8b9c-4470-9880-bb46a2fd567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(mid_atlantic)\n",
    "forest = fit_forest(X_train, y_train)\n",
    "predict_and_score(X_val, y_val, forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c88dc5-bcb5-411f-9ed6-b85b2f5a0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9f0d06-eb8a-469f-805b-bc6a39392373",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4dfff-1079-46e1-a15c-b86bee425de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    forest, X_val, y_val, n_repeats=10, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed855de-ddd6-4d72-9ca2-769944bc6d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625e1f4-5d15-46dc-9071-982efffdafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = split_data(mid_atlantic)\n",
    "forest = fit_forest(X_train, y_train)\n",
    "predict_and_score(X_val, y_val, forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26c81b-d0b9-4911-93ff-3ec4dfe021ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d6090e-2bd8-492a-b60f-cafcd4f6da4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    forest, X_val, y_val, n_repeats=10, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450f44e-bf9d-493a-bb46-1734813b2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=result.importances_std, ax=ax)\n",
    "ax.set_title(\"Feature importances using permutation\")\n",
    "ax.set_ylabel(\"Mean accuracy decrease\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d24a0c-b087-4705-99ab-0c6c78bf5b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
